from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
import whisper
import requests
import json
import os
import shutil
from datetime import datetime
import uuid
import re

app = FastAPI()

# Java Spring Boot ì„œë²„ ì£¼ì†Œ
JAVA_BACKEND_URL = "http://localhost:8080/api/orders"

# â˜…â˜…â˜… ì‚¬ìš©í•  ëª¨ë¸ëª… ê³ ì • (ì‚¬ìš©ì í™˜ê²½ì˜ ollama listì™€ ì¼ì¹˜) â˜…â˜…â˜…
TARGET_MODEL = "qwen3:14b"

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# 1. Whisper ëª¨ë¸ ë¡œë“œ
print("------------------------------------------------------")
print(f"â–¶ Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (íƒ€ê²Ÿ LLM: {TARGET_MODEL})")
stt_model = whisper.load_model("base")
print("â–¶ Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
print("------------------------------------------------------")

sessions = {}

def get_system_prompt(customer_id):
    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    return f"""
    You are a professional waiter AI at 'Mr. Daebak Dinner Service'.
    Current Date: {today}
    Customer ID: {customer_id}

    [CRITICAL INSTRUCTION]
    1. **Output ONLY a pure JSON object.** 2. **DO NOT output any thinking process (<think> tags).**
    3. The 'response' field MUST be in natural KOREAN.

    [MENU DATA]
    - Dinner Types: "valentine", "french", "english", "champagne"
    - Serving Styles: "simple", "grand", "deluxe"
    - Items (IDs): Wine(101), Steak(102), Napkin(103), Coffee(104), Salad(105), Eggs(106), Bacon(107), Bread(108), Champagne(109)

    [LOGIC]
    - Greeting -> Ask Menu -> Ask Style -> **Ask Address** -> Confirm.
    - Set "is_finished": true ONLY when order is confirmed AND address is known.

    [OUTPUT JSON FORMAT EXAMPLE]
    {{
        "response": "ì£¼ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "is_finished": true,
        "final_order": {{ 
            "customerId": {customer_id},
            "dinnerType": "valentine",
            "servingStyle": "grand",
            "deliveryAddress": "Seoul...",
            "items": [{{ "menuItemId": 102, "quantity": 1 }}]
        }}
    }}
    """

# â˜… JSON ì¶”ì¶œ í•¨ìˆ˜ (Qwen 3ì˜ <think> íƒœê·¸ ë° Markdown ì œê±°)
def extract_json_from_text(text):
    try:
        # 1. <think>...</think> íƒœê·¸ ì œê±° (ê°€ì¥ ì¤‘ìš”)
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        
        # 2. Markdown ì½”ë“œ ë¸”ëŸ­ ì œê±° (```json ... ```)
        text = text.replace("```json", "").replace("```", "")
        
        # 3. ì¤‘ê´„í˜¸ {} ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        
        if start_idx != -1 and end_idx != -1:
            json_str = text[start_idx : end_idx + 1]
            return json.loads(json_str)
        else:
            return None
    except Exception as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

@app.post("/chat")
async def chat_process(
    file: UploadFile = File(...), 
    session_id: str = Form(...),
    customer_id: int = Form(1)
):
    temp_filename = f"temp_{uuid.uuid4()}.wav"
    
    try:
        # 1. ìŒì„± íŒŒì¼ ì €ì¥
        with open(temp_filename, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
    
        # 2. STT ë³€í™˜
        print(f"\n[Processing] ìŒì„± ì¸ì‹ ì¤‘...")
        stt_result = stt_model.transcribe(temp_filename, language="ko")
        user_text = stt_result['text']
        print(f"\nğŸ—£ï¸  ì‚¬ìš©ì({session_id}): {user_text}") 

        # 3. ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
        if session_id not in sessions:
            sessions[session_id] = []
        sessions[session_id].append(f"Customer: {user_text}")
        conversation_history = "\n".join(sessions[session_id][-10:])

        # 4. LLM í˜¸ì¶œ
        full_prompt = f"{get_system_prompt(customer_id)}\n\n[Conversation History]\n{conversation_history}\n\n[Instruction]\nRespond in JSON format."
        
        print(f"ğŸ¤– AI({TARGET_MODEL})ì—ê²Œ ìš”ì²­ ì¤‘...") 
        
        # â˜… Ollama API í˜¸ì¶œ (format: "json" ì œê±°í•¨ - Qwen3 í˜¸í™˜ì„± ìœ„í•¨)
        try:
            response = requests.post('http://localhost:11434/api/generate', json={
                "model": TARGET_MODEL,  # "qwen3:14b"
                "prompt": full_prompt,
                "stream": False,
                # "format": "json",  <-- ì œê±°í•¨ (Thinking íƒœê·¸ ì„ì„ ë°©ì§€)
                "options": {
                    "temperature": 0.1, # ì •ë°€ë„ ë†’ì„
                    "num_predict": 2048 # ë‹µë³€ ì˜ë¦¼ ë°©ì§€
                }
            })
            
            if response.status_code != 200:
                print(f"âŒ Ollama ì˜¤ë¥˜ ì½”ë“œ: {response.status_code}")
                return {"status": "error", "message": f"Ollama Error: {response.text}"}

            llm_data = response.json()
            raw_response = llm_data['response']

            # â˜…â˜…â˜… [ë””ë²„ê¹…] í„°ë¯¸ë„ì—ì„œ ëª¨ë¸ì˜ ì‹¤ì œ ì‘ë‹µ í™•ì¸ â˜…â˜…â˜…
            print(f"\n========== [DEBUG: {TARGET_MODEL} Raw Output] ==========")
            print(raw_response)
            print(f"==========================================================\n")

        except Exception as e:
            print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": "Ollama Server Connection Failed"}

        # 5. ê²°ê³¼ íŒŒì‹±
        ai_response_json = extract_json_from_text(raw_response)

        if ai_response_json is None:
            ai_text = "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì‘ë‹µì„ í•´ì„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
            is_finished = False
        else:
            ai_text = ai_response_json.get("response", "ì‘ë‹µ ì—†ìŒ")
            is_finished = ai_response_json.get("is_finished", False)

        print(f"ğŸ¤– AI ë‹µë³€: {ai_text}")
        
        # 6. ë°±ì—”ë“œ ì²˜ë¦¬
        backend_status_list = []
        
        if is_finished and ai_response_json and "final_order" in ai_response_json:
            order_dto = ai_response_json["final_order"]
            print(f"ğŸ“¦ ì£¼ë¬¸ ì™„ë£Œ! ë°ì´í„°: {order_dto}")

            # ë¡œì»¬ íŒŒì¼ ì €ì¥
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                with open(f"order_{session_id}_{timestamp}.json", "w", encoding="utf-8") as f:
                    json.dump(order_dto, f, ensure_ascii=False, indent=4)
                backend_status_list.append("File Saved")
            except:
                backend_status_list.append("File Error")

            # ìë°” ë°±ì—”ë“œ ì „ì†¡
            try:
                res = requests.post(JAVA_BACKEND_URL, json=order_dto)
                if res.status_code == 200:
                    print("âœ… ë°±ì—”ë“œ ì „ì†¡ ì„±ê³µ!")
                    backend_status_list.append("Server Sent")
                else:
                    print(f"âŒ ë°±ì—”ë“œ ì „ì†¡ ì‹¤íŒ¨: {res.status_code}")
                    backend_status_list.append(f"Server Fail({res.status_code})")
            except Exception as e:
                print(f"âŒ ë°±ì—”ë“œ ì—°ê²° ì˜¤ë¥˜: {e}")
                backend_status_list.append("Conn Error")

        sessions[session_id].append(f"AI: {ai_text}")

        return {
            "status": "success",
            "user_text": user_text,
            "ai_response": ai_response_json,
            "backend_status": str(backend_status_list)
        }

    except Exception as e:
        print(f"\nâŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        return {"status": "error", "message": str(e)}
    
    finally:
        if os.path.exists(temp_filename):
            os.remove(temp_filename)

if __name__ == "__main__":
    import uvicorn
    # host="0.0.0.0"ì€ ì™¸ë¶€ ì ‘ì† í—ˆìš©, port=5000ì€ í¬íŠ¸ ë²ˆí˜¸
    print("ğŸš€ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤! (http://localhost:5000)")
    uvicorn.run(app, host="0.0.0.0", port=5000)